wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 0.7127556204795837
Loss of batch at epoch 1 : 0.711575984954834
Loss of batch at epoch 1 : 0.6876294016838074
Loss of batch at epoch 1 : 0.6711292266845703
Loss of batch at epoch 1 : 0.6855696439743042
Loss of batch at epoch 1 : 0.6786978244781494
Loss of batch at epoch 1 : 0.6647950410842896
Loss of batch at epoch 1 : 0.6513158679008484
Loss of batch at epoch 1 : 0.6599969863891602
Loss of batch at epoch 1 : 0.6669639348983765
Loss of batch at epoch 1 : 0.6542743444442749
Loss of batch at epoch 1 : 0.6494266986846924
Loss of batch at epoch 1 : 0.6420660018920898
Loss of batch at epoch 1 : 0.6426911354064941
Loss of batch at epoch 1 : 0.6511536240577698
Loss of batch at epoch 1 : 0.6465826034545898
Loss of batch at epoch 1 : 0.6344573497772217
Loss of batch at epoch 1 : 0.6204749345779419
Loss of batch at epoch 1 : 0.6295012831687927
Loss of batch at epoch 1 : 0.6210107207298279
Loss of batch at epoch 1 : 0.6157570481300354
Loss of batch at epoch 1 : 0.6159718036651611
Loss of batch at epoch 1 : 0.610600471496582
Loss of batch at epoch 1 : 0.6033638715744019
Loss of batch at epoch 1 : 0.6226017475128174
Loss of batch at epoch 1 : 0.6101258993148804
Loss of batch at epoch 1 : 0.6147774457931519
Loss of batch at epoch 1 : 0.6147415637969971
Loss of batch at epoch 1 : 0.5985256433486938
Loss of batch at epoch 1 : 0.6155543327331543
Loss of batch at epoch 1 : 0.5884701013565063
Loss of batch at epoch 1 : 0.586388349533081
Loss of batch at epoch 1 : 0.6071179509162903
Loss of batch at epoch 1 : 0.6049813032150269
Loss of batch at epoch 1 : 0.5755391120910645
Loss of batch at epoch 1 : 0.5903101563453674
Loss of batch at epoch 1 : 0.5861800909042358
Loss of batch at epoch 1 : 0.5685802698135376
Loss of batch at epoch 1 : 0.5994930863380432
Loss of batch at epoch 1 : 0.5859479904174805
Loss of batch at epoch 1 : 0.6103232502937317
Loss of batch at epoch 1 : 0.5884132385253906
Loss of batch at epoch 1 : 0.5705037117004395
Loss of batch at epoch 1 : 0.5850125551223755
Loss of batch at epoch 1 : 0.5743676424026489
Loss of batch at epoch 1 : 0.5938915014266968
Loss of batch at epoch 1 : 0.5767129063606262
Loss of batch at epoch 1 : 0.5818156003952026
Loss of batch at epoch 1 : 0.5611364841461182
Loss of batch at epoch 1 : 0.5771396160125732
Loss of batch at epoch 1 : 0.5718379020690918
Loss of batch at epoch 1 : 0.5694386959075928
Loss of batch at epoch 1 : 0.5760931372642517
Loss of batch at epoch 1 : 0.5678328275680542
Average train loss of epoch 1: 0.012435254986558592
Average validation loss of epoch 1: 0.011552288074686069
Loss of batch at epoch 2 : 0.5860214233398438
Loss of batch at epoch 2 : 0.5771588087081909
Loss of batch at epoch 2 : 0.5444004535675049
Loss of batch at epoch 2 : 0.5735025405883789
Loss of batch at epoch 2 : 0.5574314594268799
Loss of batch at epoch 2 : 0.5724865198135376
Loss of batch at epoch 2 : 0.5653977394104004
Loss of batch at epoch 2 : 0.5616284608840942
Loss of batch at epoch 2 : 0.5581148862838745
Loss of batch at epoch 2 : 0.5473371744155884
Loss of batch at epoch 2 : 0.5466210842132568
Loss of batch at epoch 2 : 0.5886422991752625
Loss of batch at epoch 2 : 0.575306236743927
Loss of batch at epoch 2 : 0.574519157409668
Loss of batch at epoch 2 : 0.5443866848945618
Loss of batch at epoch 2 : 0.5514928102493286
Loss of batch at epoch 2 : 0.5318209528923035
Loss of batch at epoch 2 : 0.5466322302818298
Loss of batch at epoch 2 : 0.5390164852142334
Loss of batch at epoch 2 : 0.5481550097465515
Loss of batch at epoch 2 : 0.541396975517273
Loss of batch at epoch 2 : 0.5579571723937988
Loss of batch at epoch 2 : 0.5316982269287109
Loss of batch at epoch 2 : 0.5386055111885071
Loss of batch at epoch 2 : 0.5511529445648193
Loss of batch at epoch 2 : 0.54363614320755
Loss of batch at epoch 2 : 0.529604434967041
Loss of batch at epoch 2 : 0.5571761131286621
Loss of batch at epoch 2 : 0.5165570974349976
Loss of batch at epoch 2 : 0.5256600379943848
Loss of batch at epoch 2 : 0.5145943760871887
Loss of batch at epoch 2 : 0.5307172536849976
Loss of batch at epoch 2 : 0.5312016606330872
Loss of batch at epoch 2 : 0.551284909248352
Loss of batch at epoch 2 : 0.5301348567008972
Loss of batch at epoch 2 : 0.517027735710144
Loss of batch at epoch 2 : 0.5158872604370117
Loss of batch at epoch 2 : 0.5253382921218872
Loss of batch at epoch 2 : 0.5463669896125793
Loss of batch at epoch 2 : 0.5326396226882935
Loss of batch at epoch 2 : 0.5066952109336853
Loss of batch at epoch 2 : 0.5524819493293762
Loss of batch at epoch 2 : 0.5118231773376465
Loss of batch at epoch 2 : 0.5162544250488281
Loss of batch at epoch 2 : 0.5234923958778381
Loss of batch at epoch 2 : 0.5274409651756287
Loss of batch at epoch 2 : 0.5328793525695801
Loss of batch at epoch 2 : 0.5400024652481079
Loss of batch at epoch 2 : 0.5290229320526123
Loss of batch at epoch 2 : 0.5183124542236328
Loss of batch at epoch 2 : 0.5249614715576172
Loss of batch at epoch 2 : 0.5169525146484375
Loss of batch at epoch 2 : 0.5306884050369263
Loss of batch at epoch 2 : 0.5458656549453735
Average train loss of epoch 2: 0.01092441763959775
Average validation loss of epoch 2: 0.01056703573926932

JOB STATISTICS
==============
Job ID: 5626685
Cluster: snellius
User/Group: scur0756/scur0756
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 07:35:21
CPU Efficiency: 61.28% of 12:23:06 core-walltime
Job Wall-clock time: 00:41:17
Memory Utilized: 86.89 GB
Memory Efficiency: 24.14% of 360.00 GB
