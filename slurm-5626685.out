wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 0.7127556204795837
Loss of batch at epoch 1 : 0.711575984954834
Loss of batch at epoch 1 : 0.6876294016838074
Loss of batch at epoch 1 : 0.6711292266845703
Loss of batch at epoch 1 : 0.6855696439743042
Loss of batch at epoch 1 : 0.6786978244781494
Loss of batch at epoch 1 : 0.6647950410842896
Loss of batch at epoch 1 : 0.6513158679008484
Loss of batch at epoch 1 : 0.6599969863891602
Loss of batch at epoch 1 : 0.6669639348983765
Loss of batch at epoch 1 : 0.6542743444442749
Loss of batch at epoch 1 : 0.6494266986846924
Loss of batch at epoch 1 : 0.6420660018920898
Loss of batch at epoch 1 : 0.6426911354064941
Loss of batch at epoch 1 : 0.6511536240577698
Loss of batch at epoch 1 : 0.6465826034545898
Loss of batch at epoch 1 : 0.6344573497772217
Loss of batch at epoch 1 : 0.6204749345779419
Loss of batch at epoch 1 : 0.6295012831687927
Loss of batch at epoch 1 : 0.6210107207298279
Loss of batch at epoch 1 : 0.6157570481300354
Loss of batch at epoch 1 : 0.6159718036651611
Loss of batch at epoch 1 : 0.610600471496582
Loss of batch at epoch 1 : 0.6033638715744019
Loss of batch at epoch 1 : 0.6226017475128174
Loss of batch at epoch 1 : 0.6101258993148804
Loss of batch at epoch 1 : 0.6147774457931519
Loss of batch at epoch 1 : 0.6147415637969971
Loss of batch at epoch 1 : 0.5985256433486938
Loss of batch at epoch 1 : 0.6155543327331543
Loss of batch at epoch 1 : 0.5884701013565063
Loss of batch at epoch 1 : 0.586388349533081
Loss of batch at epoch 1 : 0.6071179509162903
Loss of batch at epoch 1 : 0.6049813032150269
Loss of batch at epoch 1 : 0.5755391120910645
Loss of batch at epoch 1 : 0.5903101563453674
Loss of batch at epoch 1 : 0.5861800909042358
Loss of batch at epoch 1 : 0.5685802698135376
Loss of batch at epoch 1 : 0.5994930863380432
Loss of batch at epoch 1 : 0.5859479904174805
Loss of batch at epoch 1 : 0.6103232502937317
Loss of batch at epoch 1 : 0.5884132385253906
Loss of batch at epoch 1 : 0.5705037117004395
Loss of batch at epoch 1 : 0.5850125551223755
Loss of batch at epoch 1 : 0.5743676424026489
Loss of batch at epoch 1 : 0.5938915014266968
Loss of batch at epoch 1 : 0.5767129063606262
Loss of batch at epoch 1 : 0.5818156003952026
Loss of batch at epoch 1 : 0.5611364841461182
Loss of batch at epoch 1 : 0.5771396160125732
Loss of batch at epoch 1 : 0.5718379020690918
Loss of batch at epoch 1 : 0.5694386959075928
Loss of batch at epoch 1 : 0.5760931372642517
Loss of batch at epoch 1 : 0.5678328275680542
Average train loss of epoch 1: 0.012435254986558592
Average validation loss of epoch 1: 0.011552288074686069
Loss of batch at epoch 2 : 0.5860214233398438
Loss of batch at epoch 2 : 0.5771588087081909
Loss of batch at epoch 2 : 0.5444004535675049
Loss of batch at epoch 2 : 0.5735025405883789
Loss of batch at epoch 2 : 0.5574314594268799
Loss of batch at epoch 2 : 0.5724865198135376
Loss of batch at epoch 2 : 0.5653977394104004
Loss of batch at epoch 2 : 0.5616284608840942
Loss of batch at epoch 2 : 0.5581148862838745
Loss of batch at epoch 2 : 0.5473371744155884
Loss of batch at epoch 2 : 0.5466210842132568
