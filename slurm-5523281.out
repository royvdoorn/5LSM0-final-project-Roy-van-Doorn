wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.5726406574249268
Loss of batch at epoch 1 : 3.5515739917755127
Loss of batch at epoch 1 : 3.5039114952087402
Loss of batch at epoch 1 : 3.4772868156433105
Loss of batch at epoch 1 : 3.455034017562866
Loss of batch at epoch 1 : 3.4169843196868896
Loss of batch at epoch 1 : 3.3955790996551514
Loss of batch at epoch 1 : 3.3858416080474854
Loss of batch at epoch 1 : 3.3695566654205322
Loss of batch at epoch 1 : 3.337026834487915
Loss of batch at epoch 1 : 3.3225951194763184
Loss of batch at epoch 1 : 3.3458480834960938
Loss of batch at epoch 1 : 3.287675380706787
Loss of batch at epoch 1 : 3.282534599304199
Loss of batch at epoch 1 : 3.269038200378418
Loss of batch at epoch 1 : 3.2717301845550537
Loss of batch at epoch 1 : 3.255039691925049
Loss of batch at epoch 1 : 3.2318708896636963
Loss of batch at epoch 1 : 3.2444264888763428
Loss of batch at epoch 1 : 3.2250125408172607
Loss of batch at epoch 1 : 3.2136988639831543
Loss of batch at epoch 1 : 3.2074358463287354
Loss of batch at epoch 1 : 3.1949875354766846
Loss of batch at epoch 1 : 3.1847496032714844
Loss of batch at epoch 1 : 3.206808567047119
Loss of batch at epoch 1 : 3.183162212371826
Loss of batch at epoch 1 : 3.1737453937530518
Loss of batch at epoch 1 : 3.182206869125366
Loss of batch at epoch 1 : 3.157757043838501
Loss of batch at epoch 1 : 3.1876111030578613
Loss of batch at epoch 1 : 3.1430532932281494
Loss of batch at epoch 1 : 3.1215758323669434
Loss of batch at epoch 1 : 3.1400418281555176
Loss of batch at epoch 1 : 3.1222572326660156
Loss of batch at epoch 1 : 3.1376731395721436
Loss of batch at epoch 1 : 3.1066665649414062
Loss of batch at epoch 1 : 3.1064071655273438
Loss of batch at epoch 1 : 3.09480881690979
Loss of batch at epoch 1 : 3.0865609645843506
Loss of batch at epoch 1 : 3.078587770462036
Loss of batch at epoch 1 : 3.0991475582122803
Loss of batch at epoch 1 : 3.0958614349365234
Loss of batch at epoch 1 : 3.055896282196045
Loss of batch at epoch 1 : 3.0642054080963135
Loss of batch at epoch 1 : 3.048344135284424
Loss of batch at epoch 1 : 3.055664300918579
Loss of batch at epoch 1 : 3.0437073707580566
Loss of batch at epoch 1 : 3.034109115600586
Loss of batch at epoch 1 : 3.006253480911255
Loss of batch at epoch 1 : 3.0378737449645996
Loss of batch at epoch 1 : 3.022571325302124
Loss of batch at epoch 1 : 3.0025248527526855
Loss of batch at epoch 1 : 2.9980883598327637
Loss of batch at epoch 1 : 3.0146234035491943
Loss of batch at epoch 1 : 3.0017499923706055
Loss of batch at epoch 1 : 2.9833364486694336
Loss of batch at epoch 1 : 2.98475980758667
Loss of batch at epoch 1 : 2.9746792316436768
Loss of batch at epoch 1 : 2.964214324951172
Loss of batch at epoch 1 : 2.96335506439209
Loss of batch at epoch 1 : 2.968048572540283
Loss of batch at epoch 1 : 2.985975742340088
Loss of batch at epoch 1 : 2.944387674331665
Loss of batch at epoch 1 : 2.935979127883911
Loss of batch at epoch 1 : 2.9480276107788086
Loss of batch at epoch 1 : 2.915083646774292
Loss of batch at epoch 1 : 2.922895669937134
Loss of batch at epoch 1 : 2.927591323852539
Loss of batch at epoch 1 : 2.9453091621398926
Loss of batch at epoch 1 : 2.9088034629821777
Loss of batch at epoch 1 : 2.9055988788604736
Loss of batch at epoch 1 : 2.918377161026001
Loss of batch at epoch 1 : 2.907184362411499
Loss of batch at epoch 1 : 2.899170398712158
Loss of batch at epoch 1 : 2.894580841064453
Loss of batch at epoch 1 : 2.91176176071167
Loss of batch at epoch 1 : 2.904341697692871
Loss of batch at epoch 1 : 2.8908028602600098
Loss of batch at epoch 1 : 2.9001121520996094
Loss of batch at epoch 1 : 2.9013869762420654
Loss of batch at epoch 1 : 2.8907554149627686
Loss of batch at epoch 1 : 2.9027137756347656
Loss of batch at epoch 1 : 2.903212785720825
Loss of batch at epoch 1 : 2.8719606399536133
Loss of batch at epoch 1 : 2.8779332637786865
Loss of batch at epoch 1 : 2.868591070175171
Loss of batch at epoch 1 : 2.8778083324432373
Loss of batch at epoch 1 : 2.8626937866210938
Loss of batch at epoch 1 : 2.8587164878845215
Loss of batch at epoch 1 : 2.901859998703003
Loss of batch at epoch 1 : 2.891611099243164
Loss of batch at epoch 1 : 2.8557324409484863
Loss of batch at epoch 1 : 2.8630993366241455
Loss of batch at epoch 1 : 2.8436665534973145
Loss of batch at epoch 1 : 2.8617472648620605
Loss of batch at epoch 1 : 2.879986524581909
Loss of batch at epoch 1 : 2.858962297439575
Loss of batch at epoch 1 : 2.8717944622039795
Loss of batch at epoch 1 : 2.8567135334014893
Loss of batch at epoch 1 : 2.847020387649536
Loss of batch at epoch 1 : 2.8591556549072266
Loss of batch at epoch 1 : 2.847707509994507
Loss of batch at epoch 1 : 2.8544061183929443
Loss of batch at epoch 1 : 2.8353312015533447
Loss of batch at epoch 1 : 2.8451764583587646
Loss of batch at epoch 1 : 2.8363237380981445
Loss of batch at epoch 1 : 2.8433501720428467
Loss of batch at epoch 1 : 2.840589761734009
Traceback (most recent call last):
  File "/gpfs/home6/scur0756/Final_Assignment/5LSM0-final-project-Roy-van-Doorn/train.py", line 137, in <module>
    main(args)
  File "/gpfs/home6/scur0756/Final_Assignment/5LSM0-final-project-Roy-van-Doorn/train.py", line 107, in main
    loss_val = criterion(predictions, target)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 3053, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: only batches of spatial targets supported (3D tensors) but got targets of dimension: 4
[2024-03-13 11:56:11,861] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2109764) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_11:56:11
  host      : gcn28.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2109764)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: gcn28: task 0: Exited with exit code 1
srun: Terminating StepId=5523281.0

JOB STATISTICS
==============
Job ID: 5523281
Cluster: snellius
User/Group: scur0756/scur0756
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 01:22:31
CPU Efficiency: 47.26% of 02:54:36 core-walltime
Job Wall-clock time: 00:09:42
Memory Utilized: 4.91 GB
Memory Efficiency: 1.36% of 360.00 GB
