wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.799085855484009
Loss of batch at epoch 1 : 3.6449499130249023
Loss of batch at epoch 1 : 3.5323843955993652
Loss of batch at epoch 1 : 3.431535243988037
Loss of batch at epoch 1 : 3.362619161605835
Loss of batch at epoch 1 : 3.2600245475769043
Loss of batch at epoch 1 : 3.2169225215911865
Loss of batch at epoch 1 : 3.0975794792175293
Loss of batch at epoch 1 : 3.0572285652160645
Loss of batch at epoch 1 : 2.9792704582214355
Loss of batch at epoch 1 : 2.966952085494995
Loss of batch at epoch 1 : 2.900010108947754
Loss of batch at epoch 1 : 2.8683671951293945
Loss of batch at epoch 1 : 2.782118797302246
Loss of batch at epoch 1 : 2.661285400390625
Loss of batch at epoch 1 : 2.5982067584991455
Loss of batch at epoch 1 : 2.664480209350586
Loss of batch at epoch 1 : 2.5975241661071777
Loss of batch at epoch 1 : 2.5270602703094482
Loss of batch at epoch 1 : 2.5454931259155273
Loss of batch at epoch 1 : 2.4968760013580322
Loss of batch at epoch 1 : 2.496204137802124
Loss of batch at epoch 1 : 2.4416117668151855
Loss of batch at epoch 1 : 2.4410791397094727
Loss of batch at epoch 1 : 2.347700357437134
Loss of batch at epoch 1 : 2.33447527885437
Loss of batch at epoch 1 : 2.3142809867858887
Loss of batch at epoch 1 : 2.320936679840088
Loss of batch at epoch 1 : 2.2891194820404053
Loss of batch at epoch 1 : 2.230630397796631
Loss of batch at epoch 1 : 2.281747817993164
Loss of batch at epoch 1 : 2.1949236392974854
Loss of batch at epoch 1 : 2.2375874519348145
Loss of batch at epoch 1 : 2.2633731365203857
Loss of batch at epoch 1 : 2.169981002807617
Loss of batch at epoch 1 : 2.1577775478363037
Loss of batch at epoch 1 : 2.154052495956421
Loss of batch at epoch 1 : 2.055075168609619
Loss of batch at epoch 1 : 2.1107916831970215
Loss of batch at epoch 1 : 2.1850078105926514
Loss of batch at epoch 1 : 2.107675313949585
Loss of batch at epoch 1 : 2.081743001937866
Loss of batch at epoch 1 : 2.007842540740967
Loss of batch at epoch 1 : 2.092111825942993
Loss of batch at epoch 1 : 1.9432123899459839
Loss of batch at epoch 1 : 2.0088353157043457
Loss of batch at epoch 1 : 2.023881435394287
Loss of batch at epoch 1 : 2.0389182567596436
Loss of batch at epoch 1 : 1.9074939489364624
Loss of batch at epoch 1 : 1.9359357357025146
Loss of batch at epoch 1 : 1.9445034265518188
Loss of batch at epoch 1 : 2.026951551437378
Loss of batch at epoch 1 : 1.9391971826553345
Loss of batch at epoch 1 : 1.9504358768463135
Average train loss of epoch 1: 0.05004669143870484
Average validation loss of epoch 1: 0.038744239293364964
Loss of batch at epoch 2 : 1.9437872171401978
Loss of batch at epoch 2 : 1.8311775922775269
Loss of batch at epoch 2 : 1.9248437881469727
Loss of batch at epoch 2 : 1.8216567039489746
Loss of batch at epoch 2 : 1.9261201620101929
Loss of batch at epoch 2 : 1.8727021217346191
Loss of batch at epoch 2 : 1.9149070978164673
Loss of batch at epoch 2 : 1.8660467863082886
Loss of batch at epoch 2 : 1.8353737592697144
Loss of batch at epoch 2 : 1.7829201221466064
Loss of batch at epoch 2 : 1.8901969194412231
Loss of batch at epoch 2 : 1.8159953355789185
Loss of batch at epoch 2 : 1.8285011053085327
Loss of batch at epoch 2 : 1.7784754037857056
Loss of batch at epoch 2 : 1.7582170963287354
Loss of batch at epoch 2 : 1.8554785251617432
Loss of batch at epoch 2 : 1.7580119371414185
Loss of batch at epoch 2 : 1.7187352180480957
Loss of batch at epoch 2 : 1.7566585540771484
Loss of batch at epoch 2 : 1.7498887777328491
Loss of batch at epoch 2 : 1.729812502861023
Loss of batch at epoch 2 : 1.6095306873321533
Loss of batch at epoch 2 : 1.6975266933441162
Loss of batch at epoch 2 : 1.728638768196106
Loss of batch at epoch 2 : 1.7270783185958862
Loss of batch at epoch 2 : 1.7145531177520752
Loss of batch at epoch 2 : 1.6991617679595947
Loss of batch at epoch 2 : 1.718012809753418
Loss of batch at epoch 2 : 1.7372525930404663
Loss of batch at epoch 2 : 1.7047300338745117
Loss of batch at epoch 2 : 1.6691728830337524
Loss of batch at epoch 2 : 1.6544803380966187
Loss of batch at epoch 2 : 1.7097877264022827
Loss of batch at epoch 2 : 1.7283440828323364
Loss of batch at epoch 2 : 1.6643152236938477
Loss of batch at epoch 2 : 1.618200421333313
Loss of batch at epoch 2 : 1.6788338422775269
Loss of batch at epoch 2 : 1.6155182123184204
Loss of batch at epoch 2 : 1.6382290124893188
Loss of batch at epoch 2 : 1.5596855878829956
Loss of batch at epoch 2 : 1.6388187408447266
Loss of batch at epoch 2 : 1.564881682395935
Loss of batch at epoch 2 : 1.5697101354599
Loss of batch at epoch 2 : 1.5051839351654053
Loss of batch at epoch 2 : 1.5141501426696777
Loss of batch at epoch 2 : 1.513222575187683
Loss of batch at epoch 2 : 1.618080735206604
Loss of batch at epoch 2 : 1.52118980884552
Loss of batch at epoch 2 : 1.616402506828308
Loss of batch at epoch 2 : 1.632040023803711
Loss of batch at epoch 2 : 1.5678435564041138
Loss of batch at epoch 2 : 1.5596144199371338
Loss of batch at epoch 2 : 1.5648447275161743
Loss of batch at epoch 2 : 1.6569182872772217
Average train loss of epoch 2: 0.03445685274837799
Average validation loss of epoch 2: 0.03140761314417778
Loss of batch at epoch 3 : 1.637851595878601
Loss of batch at epoch 3 : 1.6236577033996582
Loss of batch at epoch 3 : 1.548682689666748
Loss of batch at epoch 3 : 1.5847349166870117
Loss of batch at epoch 3 : 1.520653247833252
Loss of batch at epoch 3 : 1.5183262825012207
Loss of batch at epoch 3 : 1.5427078008651733
Loss of batch at epoch 3 : 1.5283894538879395
Loss of batch at epoch 3 : 1.498835802078247
Loss of batch at epoch 3 : 1.5705976486206055
Loss of batch at epoch 3 : 1.453616738319397
Loss of batch at epoch 3 : 1.4966163635253906
Loss of batch at epoch 3 : 1.6218775510787964
Loss of batch at epoch 3 : 1.5000754594802856
Loss of batch at epoch 3 : 1.4813299179077148
Loss of batch at epoch 3 : 1.5172234773635864
Loss of batch at epoch 3 : 1.4249684810638428
Loss of batch at epoch 3 : 1.4859111309051514
Loss of batch at epoch 3 : 1.5821738243103027
Loss of batch at epoch 3 : 1.4428411722183228
Loss of batch at epoch 3 : 1.4655576944351196
Loss of batch at epoch 3 : 1.4451439380645752
Loss of batch at epoch 3 : 1.4765321016311646
Loss of batch at epoch 3 : 1.4403531551361084
Loss of batch at epoch 3 : 1.4256199598312378
Loss of batch at epoch 3 : 1.3904656171798706
Loss of batch at epoch 3 : 1.3955351114273071
Loss of batch at epoch 3 : 1.4989665746688843
Loss of batch at epoch 3 : 1.471667766571045
Loss of batch at epoch 3 : 1.3397490978240967
Loss of batch at epoch 3 : 1.3758111000061035
Loss of batch at epoch 3 : 1.374764323234558
Loss of batch at epoch 3 : 1.3874447345733643
Loss of batch at epoch 3 : 1.4090098142623901
Loss of batch at epoch 3 : 1.3862770795822144
Loss of batch at epoch 3 : 1.3719761371612549
Loss of batch at epoch 3 : 1.4266176223754883
Loss of batch at epoch 3 : 1.3562381267547607
Loss of batch at epoch 3 : 1.327219009399414
Loss of batch at epoch 3 : 1.4500198364257812
Loss of batch at epoch 3 : 1.3682634830474854
Loss of batch at epoch 3 : 1.3253260850906372
Loss of batch at epoch 3 : 1.3912067413330078
Loss of batch at epoch 3 : 1.364814043045044
Loss of batch at epoch 3 : 1.3205622434616089
Loss of batch at epoch 3 : 1.3182438611984253
Loss of batch at epoch 3 : 1.4277390241622925
Loss of batch at epoch 3 : 1.4030202627182007
Loss of batch at epoch 3 : 1.351399302482605
Loss of batch at epoch 3 : 1.2858620882034302
Loss of batch at epoch 3 : 1.3744559288024902
Loss of batch at epoch 3 : 1.405320405960083
Loss of batch at epoch 3 : 1.2302252054214478
Loss of batch at epoch 3 : 1.3348091840744019
Average train loss of epoch 3: 0.029013180038304005
Average validation loss of epoch 3: 0.026832662447534426
Loss of batch at epoch 4 : 1.3022817373275757
Loss of batch at epoch 4 : 1.3065422773361206
Loss of batch at epoch 4 : 1.2692794799804688
Loss of batch at epoch 4 : 1.336836338043213
Loss of batch at epoch 4 : 1.3089181184768677
Loss of batch at epoch 4 : 1.3573143482208252
Loss of batch at epoch 4 : 1.2762651443481445
Loss of batch at epoch 4 : 1.227149486541748
Loss of batch at epoch 4 : 1.353644609451294
Loss of batch at epoch 4 : 1.2570761442184448
Loss of batch at epoch 4 : 1.237364411354065
Loss of batch at epoch 4 : 1.2557274103164673
Loss of batch at epoch 4 : 1.27289617061615
Loss of batch at epoch 4 : 1.2996050119400024
Loss of batch at epoch 4 : 1.231316089630127
Loss of batch at epoch 4 : 1.2096750736236572
Loss of batch at epoch 4 : 1.1988272666931152
Loss of batch at epoch 4 : 1.2830837965011597
Loss of batch at epoch 4 : 1.1957533359527588
Loss of batch at epoch 4 : 1.2140923738479614
Loss of batch at epoch 4 : 1.2688103914260864
Loss of batch at epoch 4 : 1.311599612236023
Loss of batch at epoch 4 : 1.249279260635376
Loss of batch at epoch 4 : 1.3287147283554077
Loss of batch at epoch 4 : 1.341709017753601
Loss of batch at epoch 4 : 1.2381185293197632
Loss of batch at epoch 4 : 1.2984825372695923
Loss of batch at epoch 4 : 1.1870779991149902
Loss of batch at epoch 4 : 1.365441918373108
Loss of batch at epoch 4 : 1.3486692905426025
Loss of batch at epoch 4 : 1.2423067092895508
Loss of batch at epoch 4 : 1.2174811363220215
Loss of batch at epoch 4 : 1.21235990524292
Loss of batch at epoch 4 : 1.255553960800171
Loss of batch at epoch 4 : 1.2289412021636963
Loss of batch at epoch 4 : 1.2604680061340332
Loss of batch at epoch 4 : 1.2436261177062988
Loss of batch at epoch 4 : 1.212694525718689
Loss of batch at epoch 4 : 1.1486495733261108
Loss of batch at epoch 4 : 1.3112751245498657
Loss of batch at epoch 4 : 1.215945839881897
Loss of batch at epoch 4 : 1.2169557809829712
Loss of batch at epoch 4 : 1.249126672744751
Loss of batch at epoch 4 : 1.1852684020996094
Loss of batch at epoch 4 : 1.2478723526000977
Loss of batch at epoch 4 : 1.2397412061691284
Loss of batch at epoch 4 : 1.234373688697815
Loss of batch at epoch 4 : 1.1956732273101807
Loss of batch at epoch 4 : 1.2768893241882324
Loss of batch at epoch 4 : 1.1299446821212769
Loss of batch at epoch 4 : 1.2414603233337402
Loss of batch at epoch 4 : 1.233625054359436
Loss of batch at epoch 4 : 1.1981452703475952
Loss of batch at epoch 4 : 1.2269352674484253
Average train loss of epoch 4: 0.025301294419371966
Average validation loss of epoch 4: 0.024441226162894406
Loss of batch at epoch 5 : 1.2015403509140015
Loss of batch at epoch 5 : 1.1271418333053589
Loss of batch at epoch 5 : 1.110381841659546
Loss of batch at epoch 5 : 1.190078616142273
Loss of batch at epoch 5 : 1.2053511142730713
Loss of batch at epoch 5 : 1.1217594146728516
Loss of batch at epoch 5 : 1.206703543663025
Loss of batch at epoch 5 : 1.139230489730835
Loss of batch at epoch 5 : 1.0943694114685059
Loss of batch at epoch 5 : 1.1265074014663696
Loss of batch at epoch 5 : 1.1231120824813843
Loss of batch at epoch 5 : 1.1463066339492798
Loss of batch at epoch 5 : 1.1553577184677124
Loss of batch at epoch 5 : 1.1166939735412598
Loss of batch at epoch 5 : 1.2465407848358154
Loss of batch at epoch 5 : 1.1676719188690186
Loss of batch at epoch 5 : 1.1046828031539917
Loss of batch at epoch 5 : 1.1601841449737549
Loss of batch at epoch 5 : 1.1246994733810425
Loss of batch at epoch 5 : 1.1867694854736328
Loss of batch at epoch 5 : 1.1411805152893066
Loss of batch at epoch 5 : 1.1322053670883179
Loss of batch at epoch 5 : 1.2315998077392578
Loss of batch at epoch 5 : 1.0721241235733032
Loss of batch at epoch 5 : 1.1754145622253418
Loss of batch at epoch 5 : 1.170811653137207
Loss of batch at epoch 5 : 1.0920071601867676
Loss of batch at epoch 5 : 1.1498209238052368
Loss of batch at epoch 5 : 1.1003594398498535
Loss of batch at epoch 5 : 1.1079753637313843
Loss of batch at epoch 5 : 1.0543017387390137
Loss of batch at epoch 5 : 1.1659117937088013
Loss of batch at epoch 5 : 1.0652546882629395
Loss of batch at epoch 5 : 1.0801326036453247
Loss of batch at epoch 5 : 1.1564314365386963
Loss of batch at epoch 5 : 1.149158000946045
Loss of batch at epoch 5 : 1.1331208944320679
Loss of batch at epoch 5 : 1.1240073442459106
Loss of batch at epoch 5 : 1.190948247909546
Loss of batch at epoch 5 : 1.10193932056427
Loss of batch at epoch 5 : 1.1621677875518799
Loss of batch at epoch 5 : 1.1309906244277954
Loss of batch at epoch 5 : 1.0767440795898438
Loss of batch at epoch 5 : 1.1691608428955078
Loss of batch at epoch 5 : 1.1040149927139282
Loss of batch at epoch 5 : 1.064202070236206
Loss of batch at epoch 5 : 1.116676926612854
Loss of batch at epoch 5 : 1.1357688903808594
Loss of batch at epoch 5 : 1.123745322227478
Loss of batch at epoch 5 : 1.1379982233047485
Loss of batch at epoch 5 : 1.158893346786499
Loss of batch at epoch 5 : 1.0576332807540894
Loss of batch at epoch 5 : 1.0805412530899048
Loss of batch at epoch 5 : 1.0427030324935913
Average train loss of epoch 5: 0.02285699367167079
Average validation loss of epoch 5: 0.022371243948888297

JOB STATISTICS
==============
Job ID: 5628170
Cluster: snellius
User/Group: scur0756/scur0756
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 1-17:21:01
CPU Efficiency: 81.71% of 2-02:36:18 core-walltime
Job Wall-clock time: 02:48:41
Memory Utilized: 163.16 GB
Memory Efficiency: 45.32% of 360.00 GB
