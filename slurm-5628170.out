wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.799085855484009
Loss of batch at epoch 1 : 3.6449499130249023
Loss of batch at epoch 1 : 3.5323843955993652
Loss of batch at epoch 1 : 3.431535243988037
Loss of batch at epoch 1 : 3.362619161605835
Loss of batch at epoch 1 : 3.2600245475769043
Loss of batch at epoch 1 : 3.2169225215911865
Loss of batch at epoch 1 : 3.0975794792175293
Loss of batch at epoch 1 : 3.0572285652160645
Loss of batch at epoch 1 : 2.9792704582214355
Loss of batch at epoch 1 : 2.966952085494995
Loss of batch at epoch 1 : 2.900010108947754
Loss of batch at epoch 1 : 2.8683671951293945
Loss of batch at epoch 1 : 2.782118797302246
Loss of batch at epoch 1 : 2.661285400390625
Loss of batch at epoch 1 : 2.5982067584991455
Loss of batch at epoch 1 : 2.664480209350586
Loss of batch at epoch 1 : 2.5975241661071777
Loss of batch at epoch 1 : 2.5270602703094482
Loss of batch at epoch 1 : 2.5454931259155273
Loss of batch at epoch 1 : 2.4968760013580322
Loss of batch at epoch 1 : 2.496204137802124
Loss of batch at epoch 1 : 2.4416117668151855
Loss of batch at epoch 1 : 2.4410791397094727
Loss of batch at epoch 1 : 2.347700357437134
Loss of batch at epoch 1 : 2.33447527885437
Loss of batch at epoch 1 : 2.3142809867858887
Loss of batch at epoch 1 : 2.320936679840088
Loss of batch at epoch 1 : 2.2891194820404053
Loss of batch at epoch 1 : 2.230630397796631
Loss of batch at epoch 1 : 2.281747817993164
Loss of batch at epoch 1 : 2.1949236392974854
Loss of batch at epoch 1 : 2.2375874519348145
Loss of batch at epoch 1 : 2.2633731365203857
Loss of batch at epoch 1 : 2.169981002807617
Loss of batch at epoch 1 : 2.1577775478363037
Loss of batch at epoch 1 : 2.154052495956421
Loss of batch at epoch 1 : 2.055075168609619
Loss of batch at epoch 1 : 2.1107916831970215
Loss of batch at epoch 1 : 2.1850078105926514
Loss of batch at epoch 1 : 2.107675313949585
Loss of batch at epoch 1 : 2.081743001937866
Loss of batch at epoch 1 : 2.007842540740967
Loss of batch at epoch 1 : 2.092111825942993
Loss of batch at epoch 1 : 1.9432123899459839
Loss of batch at epoch 1 : 2.0088353157043457
Loss of batch at epoch 1 : 2.023881435394287
Loss of batch at epoch 1 : 2.0389182567596436
Loss of batch at epoch 1 : 1.9074939489364624
Loss of batch at epoch 1 : 1.9359357357025146
Loss of batch at epoch 1 : 1.9445034265518188
Loss of batch at epoch 1 : 2.026951551437378
Loss of batch at epoch 1 : 1.9391971826553345
Loss of batch at epoch 1 : 1.9504358768463135
Average train loss of epoch 1: 0.05004669143870484
Average validation loss of epoch 1: 0.038744239293364964
Loss of batch at epoch 2 : 1.9437872171401978
Loss of batch at epoch 2 : 1.8311775922775269
Loss of batch at epoch 2 : 1.9248437881469727
Loss of batch at epoch 2 : 1.8216567039489746
Loss of batch at epoch 2 : 1.9261201620101929
Loss of batch at epoch 2 : 1.8727021217346191
Loss of batch at epoch 2 : 1.9149070978164673
Loss of batch at epoch 2 : 1.8660467863082886
Loss of batch at epoch 2 : 1.8353737592697144
Loss of batch at epoch 2 : 1.7829201221466064
Loss of batch at epoch 2 : 1.8901969194412231
Loss of batch at epoch 2 : 1.8159953355789185
Loss of batch at epoch 2 : 1.8285011053085327
Loss of batch at epoch 2 : 1.7784754037857056
Loss of batch at epoch 2 : 1.7582170963287354
Loss of batch at epoch 2 : 1.8554785251617432
Loss of batch at epoch 2 : 1.7580119371414185
Loss of batch at epoch 2 : 1.7187352180480957
Loss of batch at epoch 2 : 1.7566585540771484
Loss of batch at epoch 2 : 1.7498887777328491
Loss of batch at epoch 2 : 1.729812502861023
Loss of batch at epoch 2 : 1.6095306873321533
Loss of batch at epoch 2 : 1.6975266933441162
Loss of batch at epoch 2 : 1.728638768196106
Loss of batch at epoch 2 : 1.7270783185958862
Loss of batch at epoch 2 : 1.7145531177520752
Loss of batch at epoch 2 : 1.6991617679595947
Loss of batch at epoch 2 : 1.718012809753418
Loss of batch at epoch 2 : 1.7372525930404663
Loss of batch at epoch 2 : 1.7047300338745117
Loss of batch at epoch 2 : 1.6691728830337524
Loss of batch at epoch 2 : 1.6544803380966187
Loss of batch at epoch 2 : 1.7097877264022827
Loss of batch at epoch 2 : 1.7283440828323364
Loss of batch at epoch 2 : 1.6643152236938477
Loss of batch at epoch 2 : 1.618200421333313
Loss of batch at epoch 2 : 1.6788338422775269
Loss of batch at epoch 2 : 1.6155182123184204
Loss of batch at epoch 2 : 1.6382290124893188
Loss of batch at epoch 2 : 1.5596855878829956
Loss of batch at epoch 2 : 1.6388187408447266
Loss of batch at epoch 2 : 1.564881682395935
Loss of batch at epoch 2 : 1.5697101354599
Loss of batch at epoch 2 : 1.5051839351654053
Loss of batch at epoch 2 : 1.5141501426696777
Loss of batch at epoch 2 : 1.513222575187683
Loss of batch at epoch 2 : 1.618080735206604
Loss of batch at epoch 2 : 1.52118980884552
Loss of batch at epoch 2 : 1.616402506828308
Loss of batch at epoch 2 : 1.632040023803711
Loss of batch at epoch 2 : 1.5678435564041138
Loss of batch at epoch 2 : 1.5596144199371338
Loss of batch at epoch 2 : 1.5648447275161743
Loss of batch at epoch 2 : 1.6569182872772217
Average train loss of epoch 2: 0.03445685274837799
Average validation loss of epoch 2: 0.03140761314417778
Loss of batch at epoch 3 : 1.637851595878601
Loss of batch at epoch 3 : 1.6236577033996582
Loss of batch at epoch 3 : 1.548682689666748
Loss of batch at epoch 3 : 1.5847349166870117
Loss of batch at epoch 3 : 1.520653247833252
Loss of batch at epoch 3 : 1.5183262825012207
Loss of batch at epoch 3 : 1.5427078008651733
Loss of batch at epoch 3 : 1.5283894538879395
Loss of batch at epoch 3 : 1.498835802078247
Loss of batch at epoch 3 : 1.5705976486206055
Loss of batch at epoch 3 : 1.453616738319397
Loss of batch at epoch 3 : 1.4966163635253906
Loss of batch at epoch 3 : 1.6218775510787964
Loss of batch at epoch 3 : 1.5000754594802856
Loss of batch at epoch 3 : 1.4813299179077148
