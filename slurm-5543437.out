wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.5092906951904297
Loss of batch at epoch 1 : 3.1966943740844727
Loss of batch at epoch 1 : 2.842264175415039
Loss of batch at epoch 1 : 2.5830535888671875
Loss of batch at epoch 1 : 2.453411102294922
Loss of batch at epoch 1 : 2.453122854232788
Loss of batch at epoch 1 : 2.176107168197632
Loss of batch at epoch 1 : 2.069423198699951
Loss of batch at epoch 1 : 2.00395131111145
Loss of batch at epoch 1 : 2.1019909381866455
Loss of batch at epoch 1 : 2.0555388927459717
Loss of batch at epoch 1 : 1.84525465965271
Loss of batch at epoch 1 : 1.957590103149414
Loss of batch at epoch 1 : 1.8451217412948608
Loss of batch at epoch 1 : 1.75458824634552
Loss of batch at epoch 1 : 1.7923412322998047
Loss of batch at epoch 1 : 1.6703407764434814
Loss of batch at epoch 1 : 1.569563388824463
Loss of batch at epoch 1 : 1.5505560636520386
Loss of batch at epoch 1 : 1.5457621812820435
Loss of batch at epoch 1 : 1.6508686542510986
Loss of batch at epoch 1 : 1.5473535060882568
Loss of batch at epoch 1 : 1.4648581743240356
Loss of batch at epoch 1 : 1.4742095470428467
Loss of batch at epoch 1 : 1.4012954235076904
Loss of batch at epoch 1 : 1.409946322441101
Loss of batch at epoch 1 : 1.4922574758529663
Loss of batch at epoch 1 : 1.4466553926467896
Loss of batch at epoch 1 : 1.3995606899261475
Loss of batch at epoch 1 : 1.349163293838501
Loss of batch at epoch 1 : 1.4341819286346436
Loss of batch at epoch 1 : 1.348349928855896
Loss of batch at epoch 1 : 1.433876633644104
Loss of batch at epoch 1 : 1.213804841041565
Loss of batch at epoch 1 : 1.2711106538772583
Loss of batch at epoch 1 : 1.2530109882354736
Loss of batch at epoch 1 : 1.274879813194275
Loss of batch at epoch 1 : 1.217667579650879
Loss of batch at epoch 1 : 1.239489197731018
Loss of batch at epoch 1 : 1.1843976974487305
Loss of batch at epoch 1 : 1.1476548910140991
Loss of batch at epoch 1 : 1.1841773986816406
Loss of batch at epoch 1 : 1.091301679611206
Loss of batch at epoch 1 : 1.1618894338607788
Loss of batch at epoch 1 : 1.0637915134429932
Loss of batch at epoch 1 : 1.0199095010757446
Loss of batch at epoch 1 : 1.0489414930343628
Loss of batch at epoch 1 : 0.9971432089805603
Loss of batch at epoch 1 : 1.0533398389816284
Loss of batch at epoch 1 : 1.0840736627578735
Loss of batch at epoch 1 : 1.1907240152359009
Loss of batch at epoch 1 : 1.0476126670837402
Loss of batch at epoch 1 : 0.9545813798904419
Loss of batch at epoch 1 : 1.02357816696167
Loss of batch at epoch 1 : 1.0111480951309204
Loss of batch at epoch 1 : 1.0431469678878784
Loss of batch at epoch 1 : 0.9833725690841675
Loss of batch at epoch 1 : 0.9754752516746521
Loss of batch at epoch 1 : 1.0506422519683838
Loss of batch at epoch 1 : 1.0053287744522095
Loss of batch at epoch 1 : 0.9949597716331482
Loss of batch at epoch 1 : 0.9853048324584961
Loss of batch at epoch 1 : 0.8811750411987305
Loss of batch at epoch 1 : 0.8799992799758911
Loss of batch at epoch 1 : 0.9211236238479614
Loss of batch at epoch 1 : 0.9178432822227478
Loss of batch at epoch 1 : 0.855786144733429
Loss of batch at epoch 1 : 0.9211813807487488
Loss of batch at epoch 1 : 0.85371994972229
Loss of batch at epoch 1 : 0.8702704906463623
Loss of batch at epoch 1 : 0.9574184417724609
Loss of batch at epoch 1 : 1.07347571849823
Loss of batch at epoch 1 : 0.8744468688964844
Loss of batch at epoch 1 : 0.8641120195388794
Loss of batch at epoch 1 : 0.818890392780304
Loss of batch at epoch 1 : 0.847242534160614
Loss of batch at epoch 1 : 0.8275164365768433
Loss of batch at epoch 1 : 0.801798403263092
Loss of batch at epoch 1 : 0.911922037601471
Loss of batch at epoch 1 : 0.912458598613739
Loss of batch at epoch 1 : 1.0268405675888062
Loss of batch at epoch 1 : 0.8416778445243835
Loss of batch at epoch 1 : 0.9013954401016235
Loss of batch at epoch 1 : 0.9544619917869568
Loss of batch at epoch 1 : 0.8582897782325745
Loss of batch at epoch 1 : 0.8606818914413452
Loss of batch at epoch 1 : 0.8502604365348816
Loss of batch at epoch 1 : 0.8355043530464172
Loss of batch at epoch 1 : 0.8617650270462036
Loss of batch at epoch 1 : 0.9747219085693359
Loss of batch at epoch 1 : 0.8000308871269226
Loss of batch at epoch 1 : 0.9180196523666382
Loss of batch at epoch 1 : 0.80484938621521
Loss of batch at epoch 1 : 0.8211105465888977
Loss of batch at epoch 1 : 0.8197558522224426
Loss of batch at epoch 1 : 0.8024849891662598
Loss of batch at epoch 1 : 0.8369743227958679
Loss of batch at epoch 1 : 0.8219495415687561
Loss of batch at epoch 1 : 0.8892370462417603
Loss of batch at epoch 1 : 0.8068296909332275
Loss of batch at epoch 1 : 0.7688705921173096
Loss of batch at epoch 1 : 0.7553194165229797
Loss of batch at epoch 1 : 0.9601420164108276
Loss of batch at epoch 1 : 0.8507998585700989
Loss of batch at epoch 1 : 0.7832885980606079
Loss of batch at epoch 1 : 0.7960925102233887
Loss of batch at epoch 1 : 0.8140181303024292
Loss of batch at epoch 1 : 0.850649893283844
Average train loss of epoch 1: 0.049834726460394885
Average validation loss of epoch 1: 0.033046304978906905
Loss of batch at epoch 2 : 0.8541184067726135
Loss of batch at epoch 2 : 0.8482880592346191
Loss of batch at epoch 2 : 0.8550733327865601
Loss of batch at epoch 2 : 0.8100830912590027
Loss of batch at epoch 2 : 0.8900359272956848
Loss of batch at epoch 2 : 0.8950321674346924
Loss of batch at epoch 2 : 0.9026918411254883
Loss of batch at epoch 2 : 0.6991541981697083
Loss of batch at epoch 2 : 0.7983961701393127
Loss of batch at epoch 2 : 0.7731548547744751
Loss of batch at epoch 2 : 0.8193272948265076
Loss of batch at epoch 2 : 0.7426849603652954
Loss of batch at epoch 2 : 0.8038445711135864
Loss of batch at epoch 2 : 0.7394750118255615
Loss of batch at epoch 2 : 0.8942757248878479
Loss of batch at epoch 2 : 0.830337643623352
Loss of batch at epoch 2 : 0.7446829676628113
Loss of batch at epoch 2 : 0.7205487489700317
Loss of batch at epoch 2 : 0.7330989837646484
Loss of batch at epoch 2 : 0.7945393919944763
Loss of batch at epoch 2 : 0.8573480844497681
Loss of batch at epoch 2 : 0.7364948391914368
Loss of batch at epoch 2 : 0.8244814872741699
Loss of batch at epoch 2 : 0.7094804644584656
Loss of batch at epoch 2 : 0.8273428678512573
Loss of batch at epoch 2 : 0.8051130175590515
Loss of batch at epoch 2 : 0.728572428226471
Loss of batch at epoch 2 : 0.776483952999115
Loss of batch at epoch 2 : 0.6983212828636169
Loss of batch at epoch 2 : 0.699286937713623
Loss of batch at epoch 2 : 0.7440488934516907
Loss of batch at epoch 2 : 0.6512624621391296
Loss of batch at epoch 2 : 0.7226872444152832
Loss of batch at epoch 2 : 0.7185066342353821
Loss of batch at epoch 2 : 0.7700183391571045
Loss of batch at epoch 2 : 0.7430880069732666
Loss of batch at epoch 2 : 0.7450637221336365
Loss of batch at epoch 2 : 0.8090406656265259
Loss of batch at epoch 2 : 0.8727517127990723
Loss of batch at epoch 2 : 0.6767232418060303
Loss of batch at epoch 2 : 0.8376339077949524
Loss of batch at epoch 2 : 0.7478544116020203
Loss of batch at epoch 2 : 0.7369081377983093
Loss of batch at epoch 2 : 0.6793239116668701
Loss of batch at epoch 2 : 0.7652069926261902
Loss of batch at epoch 2 : 0.719385027885437
Loss of batch at epoch 2 : 0.6775864958763123
Loss of batch at epoch 2 : 0.6790599822998047
Loss of batch at epoch 2 : 0.7658216953277588
Loss of batch at epoch 2 : 0.7745328545570374
Loss of batch at epoch 2 : 0.7189203500747681
Loss of batch at epoch 2 : 0.685168445110321
Loss of batch at epoch 2 : 0.7331072092056274
Loss of batch at epoch 2 : 0.8452290296554565
Loss of batch at epoch 2 : 0.6749429106712341
Loss of batch at epoch 2 : 0.7844178080558777
Loss of batch at epoch 2 : 0.6973801851272583
Loss of batch at epoch 2 : 0.750525176525116
Loss of batch at epoch 2 : 0.6238842010498047
Loss of batch at epoch 2 : 0.7047069668769836
Loss of batch at epoch 2 : 0.6936236023902893
Loss of batch at epoch 2 : 0.7062622904777527
Loss of batch at epoch 2 : 0.760927140712738
Loss of batch at epoch 2 : 0.7157108783721924
Loss of batch at epoch 2 : 0.7291303277015686
Loss of batch at epoch 2 : 0.7727856040000916
Loss of batch at epoch 2 : 0.8085329532623291
Loss of batch at epoch 2 : 0.6568467020988464
Loss of batch at epoch 2 : 0.7376036047935486
Loss of batch at epoch 2 : 0.6538984179496765
Loss of batch at epoch 2 : 0.7010245323181152
Loss of batch at epoch 2 : 0.6476444602012634
Loss of batch at epoch 2 : 0.6378260850906372
Loss of batch at epoch 2 : 0.6373351216316223
Loss of batch at epoch 2 : 0.6354785561561584
Loss of batch at epoch 2 : 0.7202931642532349
Loss of batch at epoch 2 : 0.5876785516738892
Loss of batch at epoch 2 : 0.8319664001464844
slurmstepd: error: *** STEP 5543437.0 ON gcn51 CANCELLED AT 2024-03-14T16:43:57 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 5543437 ON gcn51 CANCELLED AT 2024-03-14T16:43:57 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: container_p_join: setns failed for /slurm/5543437/.ns: Invalid argument
slurmstepd: error: container_g_join(5543437): Invalid argument

JOB STATISTICS
==============
Job ID: 5543437
Cluster: snellius
User/Group: scur0756/scur0756
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:10
CPU Efficiency: 0.03% of 09:03:36 core-walltime
Job Wall-clock time: 00:30:12
Memory Utilized: 60.42 GB
Memory Efficiency: 16.78% of 360.00 GB
