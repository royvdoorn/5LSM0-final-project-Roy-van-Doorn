wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 0.9186159372329712
Loss of batch at epoch 1 : 0.9063513875007629
Loss of batch at epoch 1 : 0.898714542388916
Loss of batch at epoch 1 : 0.888862133026123
Loss of batch at epoch 1 : 0.8869774341583252
Loss of batch at epoch 1 : 0.8880674839019775
Loss of batch at epoch 1 : 0.8795775771141052
Loss of batch at epoch 1 : 0.8845362663269043
Loss of batch at epoch 1 : 0.8838880062103271
Loss of batch at epoch 1 : 0.8849156498908997
Loss of batch at epoch 1 : 0.8789646625518799
Loss of batch at epoch 1 : 0.8818930983543396
Loss of batch at epoch 1 : 0.8787012696266174
Loss of batch at epoch 1 : 0.8786799907684326
Loss of batch at epoch 1 : 0.88325434923172
Loss of batch at epoch 1 : 0.8756886720657349
Loss of batch at epoch 1 : 0.8807776570320129
Loss of batch at epoch 1 : 0.8796036243438721
Loss of batch at epoch 1 : 0.8765977025032043
Loss of batch at epoch 1 : 0.8748422265052795
Loss of batch at epoch 1 : 0.8765685558319092
Loss of batch at epoch 1 : 0.8836364150047302
Loss of batch at epoch 1 : 0.8743923902511597
Loss of batch at epoch 1 : 0.876669704914093
Loss of batch at epoch 1 : 0.8764558434486389
Loss of batch at epoch 1 : 0.871787428855896
Loss of batch at epoch 1 : 0.8777710199356079
Loss of batch at epoch 1 : 0.8774356245994568
Loss of batch at epoch 1 : 0.8799005150794983
Loss of batch at epoch 1 : 0.8705174326896667
Loss of batch at epoch 1 : 0.8793936967849731
Loss of batch at epoch 1 : 0.8689556121826172
Loss of batch at epoch 1 : 0.876158595085144
Loss of batch at epoch 1 : 0.8769086003303528
Loss of batch at epoch 1 : 0.8749575614929199
Loss of batch at epoch 1 : 0.8801979422569275
Loss of batch at epoch 1 : 0.8773109912872314
Loss of batch at epoch 1 : 0.8754143714904785
Loss of batch at epoch 1 : 0.8691567182540894
Loss of batch at epoch 1 : 0.8764253258705139
Loss of batch at epoch 1 : 0.877048671245575
Loss of batch at epoch 1 : 0.8718018531799316
Loss of batch at epoch 1 : 0.8692566156387329
Loss of batch at epoch 1 : 0.8727262020111084
Loss of batch at epoch 1 : 0.8759365677833557
Loss of batch at epoch 1 : 0.8746724724769592
Loss of batch at epoch 1 : 0.8698633909225464
Loss of batch at epoch 1 : 0.8785864114761353
Loss of batch at epoch 1 : 0.8731285929679871
Loss of batch at epoch 1 : 0.8719991445541382
Loss of batch at epoch 1 : 0.8699601888656616
Loss of batch at epoch 1 : 0.8703486919403076
Loss of batch at epoch 1 : 0.8664977550506592
Loss of batch at epoch 1 : 0.8788846731185913
Loss of batch at epoch 1 : 0.8675630688667297
Loss of batch at epoch 1 : 0.8675307035446167
Loss of batch at epoch 1 : 0.8718820214271545
Loss of batch at epoch 1 : 0.8714775443077087
Loss of batch at epoch 1 : 0.8702703714370728
Loss of batch at epoch 1 : 0.8821276426315308
Loss of batch at epoch 1 : 0.8774929642677307
Loss of batch at epoch 1 : 0.8691126704216003
Loss of batch at epoch 1 : 0.8661602139472961
Loss of batch at epoch 1 : 0.8708100318908691
Loss of batch at epoch 1 : 0.8706688284873962
Loss of batch at epoch 1 : 0.873089075088501
Loss of batch at epoch 1 : 0.8718256950378418
Loss of batch at epoch 1 : 0.8671830892562866
Loss of batch at epoch 1 : 0.8732961416244507
Loss of batch at epoch 1 : 0.8697038888931274
Loss of batch at epoch 1 : 0.8721253871917725
Loss of batch at epoch 1 : 0.8704285621643066
Loss of batch at epoch 1 : 0.8714600801467896
Loss of batch at epoch 1 : 0.8714267015457153
Loss of batch at epoch 1 : 0.8697857856750488
Loss of batch at epoch 1 : 0.8719720840454102
Loss of batch at epoch 1 : 0.8729200959205627
Loss of batch at epoch 1 : 0.8738347291946411
Loss of batch at epoch 1 : 0.870914101600647
Loss of batch at epoch 1 : 0.869255542755127
Loss of batch at epoch 1 : 0.8834700584411621
Loss of batch at epoch 1 : 0.8721739649772644
Loss of batch at epoch 1 : 0.8687440156936646
Loss of batch at epoch 1 : 0.8719865083694458
Loss of batch at epoch 1 : 0.8732156753540039
Loss of batch at epoch 1 : 0.8694956302642822
Loss of batch at epoch 1 : 0.8726295828819275
Loss of batch at epoch 1 : 0.8737281560897827
Loss of batch at epoch 1 : 0.8762081861495972
Loss of batch at epoch 1 : 0.8709900379180908
Loss of batch at epoch 1 : 0.8703921437263489
Loss of batch at epoch 1 : 0.86561518907547
Loss of batch at epoch 1 : 0.8723597526550293
Loss of batch at epoch 1 : 0.8700928688049316
Loss of batch at epoch 1 : 0.8694311380386353
Loss of batch at epoch 1 : 0.8673663139343262
Loss of batch at epoch 1 : 0.8725552558898926
Loss of batch at epoch 1 : 0.8675313591957092
Loss of batch at epoch 1 : 0.8677759170532227
Loss of batch at epoch 1 : 0.8708533644676208
Loss of batch at epoch 1 : 0.8694272041320801
Loss of batch at epoch 1 : 0.8700924515724182
