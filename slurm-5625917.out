wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.547395944595337
Loss of batch at epoch 1 : 3.42578387260437
Loss of batch at epoch 1 : 3.3305671215057373
Loss of batch at epoch 1 : 3.2558984756469727
Loss of batch at epoch 1 : 3.1718568801879883
Loss of batch at epoch 1 : 3.076885461807251
Loss of batch at epoch 1 : 3.005192518234253
Loss of batch at epoch 1 : 2.9559714794158936
Loss of batch at epoch 1 : 2.8631622791290283
Loss of batch at epoch 1 : 2.8597254753112793
Loss of batch at epoch 1 : 2.7740304470062256
Loss of batch at epoch 1 : 2.741874933242798
Loss of batch at epoch 1 : 2.6928727626800537
Loss of batch at epoch 1 : 2.646346092224121
Loss of batch at epoch 1 : 2.6019680500030518
Loss of batch at epoch 1 : 2.5307517051696777
Loss of batch at epoch 1 : 2.5493714809417725
Loss of batch at epoch 1 : 2.4814841747283936
Loss of batch at epoch 1 : 2.3948936462402344
Loss of batch at epoch 1 : 2.403855800628662
Loss of batch at epoch 1 : 2.3768088817596436
Loss of batch at epoch 1 : 2.326385498046875
Loss of batch at epoch 1 : 2.282595157623291
Loss of batch at epoch 1 : 2.306739091873169
Loss of batch at epoch 1 : 2.20172119140625
Loss of batch at epoch 1 : 2.3228771686553955
Loss of batch at epoch 1 : 2.1394031047821045
Loss of batch at epoch 1 : 2.142449378967285
Loss of batch at epoch 1 : 2.1116254329681396
Loss of batch at epoch 1 : 2.1545233726501465
Loss of batch at epoch 1 : 2.090181350708008
Loss of batch at epoch 1 : 2.1134321689605713
Loss of batch at epoch 1 : 2.0687255859375
Loss of batch at epoch 1 : 2.070519208908081
Loss of batch at epoch 1 : 2.0334601402282715
Loss of batch at epoch 1 : 2.0108988285064697
Loss of batch at epoch 1 : 1.9630173444747925
Loss of batch at epoch 1 : 2.009758472442627
Loss of batch at epoch 1 : 1.9715551137924194
Loss of batch at epoch 1 : 1.9688186645507812
Loss of batch at epoch 1 : 1.954167366027832
Loss of batch at epoch 1 : 1.939681887626648
Loss of batch at epoch 1 : 1.9090126752853394
Loss of batch at epoch 1 : 1.953593134880066
Loss of batch at epoch 1 : 1.942923665046692
Loss of batch at epoch 1 : 1.8998278379440308
Loss of batch at epoch 1 : 1.849648118019104
Loss of batch at epoch 1 : 1.8844488859176636
Loss of batch at epoch 1 : 1.85335373878479
Loss of batch at epoch 1 : 1.8264588117599487
Loss of batch at epoch 1 : 1.886414647102356
Loss of batch at epoch 1 : 1.8581411838531494
Loss of batch at epoch 1 : 1.8565229177474976
Loss of batch at epoch 1 : 1.8715227842330933
Average train loss of epoch 1: 0.04722221722079356
Average validation loss of epoch 1: 0.03709033683494285
Loss of batch at epoch 2 : 1.8138294219970703
Loss of batch at epoch 2 : 1.8462878465652466
Loss of batch at epoch 2 : 1.8404004573822021
Loss of batch at epoch 2 : 1.725973129272461
Loss of batch at epoch 2 : 1.818157434463501
Loss of batch at epoch 2 : 1.7613844871520996
Loss of batch at epoch 2 : 1.7311969995498657
Loss of batch at epoch 2 : 1.7833141088485718
Loss of batch at epoch 2 : 1.7570098638534546
Loss of batch at epoch 2 : 1.7934876680374146
Loss of batch at epoch 2 : 1.7461333274841309
Loss of batch at epoch 2 : 1.7200565338134766
Loss of batch at epoch 2 : 1.7603414058685303
Loss of batch at epoch 2 : 1.692566156387329
Loss of batch at epoch 2 : 1.6909668445587158
Loss of batch at epoch 2 : 1.7027628421783447
Loss of batch at epoch 2 : 1.7062163352966309
Loss of batch at epoch 2 : 1.7217817306518555
Loss of batch at epoch 2 : 1.752045750617981
Loss of batch at epoch 2 : 1.7214691638946533
Loss of batch at epoch 2 : 1.7146992683410645
Loss of batch at epoch 2 : 1.722208857536316
Loss of batch at epoch 2 : 1.6808756589889526
Loss of batch at epoch 2 : 1.775765299797058
Loss of batch at epoch 2 : 1.6992543935775757
Loss of batch at epoch 2 : 1.647158145904541
Loss of batch at epoch 2 : 1.6733291149139404
Loss of batch at epoch 2 : 1.699302315711975
Loss of batch at epoch 2 : 1.7155892848968506
Loss of batch at epoch 2 : 1.6455466747283936
Loss of batch at epoch 2 : 1.6539443731307983
Loss of batch at epoch 2 : 1.6145987510681152
Loss of batch at epoch 2 : 1.57030189037323
Loss of batch at epoch 2 : 1.6953750848770142
Loss of batch at epoch 2 : 1.6845506429672241
Loss of batch at epoch 2 : 1.643804669380188
Loss of batch at epoch 2 : 1.62136971950531
Loss of batch at epoch 2 : 1.6189464330673218
Loss of batch at epoch 2 : 1.5958948135375977
Loss of batch at epoch 2 : 1.628272533416748
Loss of batch at epoch 2 : 1.577127456665039
Loss of batch at epoch 2 : 1.5551767349243164
Loss of batch at epoch 2 : 1.6320114135742188
Loss of batch at epoch 2 : 1.5267515182495117
Loss of batch at epoch 2 : 1.580085039138794
Loss of batch at epoch 2 : 1.6355851888656616
Loss of batch at epoch 2 : 1.5728892087936401
Loss of batch at epoch 2 : 1.6157972812652588
Loss of batch at epoch 2 : 1.565826177597046
Loss of batch at epoch 2 : 1.5346171855926514
Loss of batch at epoch 2 : 1.5692931413650513
Loss of batch at epoch 2 : 1.55524742603302
Loss of batch at epoch 2 : 1.4782658815383911
Loss of batch at epoch 2 : 1.745011329650879
Average train loss of epoch 2: 0.033805015284771166
Average validation loss of epoch 2: 0.031119523225007235
Loss of batch at epoch 3 : 1.4820330142974854
Loss of batch at epoch 3 : 1.4717962741851807
Loss of batch at epoch 3 : 1.5201225280761719
Loss of batch at epoch 3 : 1.4981849193572998
Loss of batch at epoch 3 : 1.490106463432312
Loss of batch at epoch 3 : 1.4902899265289307
Loss of batch at epoch 3 : 1.4944911003112793
Loss of batch at epoch 3 : 1.501584768295288
Loss of batch at epoch 3 : 1.5150467157363892
Loss of batch at epoch 3 : 1.5263313055038452
Loss of batch at epoch 3 : 1.4335535764694214
Loss of batch at epoch 3 : 1.5284039974212646
Loss of batch at epoch 3 : 1.4777477979660034
Loss of batch at epoch 3 : 1.4743961095809937
Loss of batch at epoch 3 : 1.4709668159484863
Loss of batch at epoch 3 : 1.4303038120269775
Loss of batch at epoch 3 : 1.4565646648406982
Loss of batch at epoch 3 : 1.5238654613494873
Loss of batch at epoch 3 : 1.4523353576660156
Loss of batch at epoch 3 : 1.499672770500183
Loss of batch at epoch 3 : 1.4385206699371338
Loss of batch at epoch 3 : 1.533378005027771
Loss of batch at epoch 3 : 1.4784334897994995
Loss of batch at epoch 3 : 1.4784321784973145
Loss of batch at epoch 3 : 1.4932074546813965
Loss of batch at epoch 3 : 1.4680531024932861
Loss of batch at epoch 3 : 1.450310468673706
Loss of batch at epoch 3 : 1.4283374547958374
Loss of batch at epoch 3 : 1.3960031270980835
Loss of batch at epoch 3 : 1.4418830871582031
Loss of batch at epoch 3 : 1.364112138748169
Loss of batch at epoch 3 : 1.4132789373397827
Loss of batch at epoch 3 : 1.352411150932312
Loss of batch at epoch 3 : 1.378056526184082
Loss of batch at epoch 3 : 1.364647388458252
Loss of batch at epoch 3 : 1.3171340227127075
Loss of batch at epoch 3 : 1.3325419425964355
Loss of batch at epoch 3 : 1.3799265623092651
Loss of batch at epoch 3 : 1.3622334003448486
Loss of batch at epoch 3 : 1.4324250221252441
Loss of batch at epoch 3 : 1.3778877258300781
Loss of batch at epoch 3 : 1.4012335538864136
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 5625917.0 ON gcn34 CANCELLED AT 2024-03-21T10:31:31 ***
slurmstepd: error: *** JOB 5625917 ON gcn34 CANCELLED AT 2024-03-21T10:31:31 ***

JOB STATISTICS
==============
Job ID: 5625917
Cluster: snellius
User/Group: scur0756/scur0756
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:10
CPU Efficiency: 0.02% of 16:35:24 core-walltime
Job Wall-clock time: 00:55:18
Memory Utilized: 69.45 GB
Memory Efficiency: 19.29% of 360.00 GB
