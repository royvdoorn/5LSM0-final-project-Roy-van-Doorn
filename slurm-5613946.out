wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.825331687927246
Loss of batch at epoch 1 : 3.6761958599090576
Loss of batch at epoch 1 : 3.5458545684814453
Loss of batch at epoch 1 : 3.461430549621582
Loss of batch at epoch 1 : 3.355281114578247
Loss of batch at epoch 1 : 3.3251094818115234
Loss of batch at epoch 1 : 3.2930221557617188
Loss of batch at epoch 1 : 3.225851535797119
Loss of batch at epoch 1 : 3.0998213291168213
Loss of batch at epoch 1 : 3.08937406539917
Loss of batch at epoch 1 : 3.0398640632629395
Loss of batch at epoch 1 : 2.9644787311553955
Loss of batch at epoch 1 : 2.9198083877563477
Loss of batch at epoch 1 : 2.845775604248047
Loss of batch at epoch 1 : 2.861098289489746
Loss of batch at epoch 1 : 2.8442983627319336
Loss of batch at epoch 1 : 2.7289552688598633
Loss of batch at epoch 1 : 2.6885201930999756
Loss of batch at epoch 1 : 2.6196210384368896
Loss of batch at epoch 1 : 2.612666606903076
Loss of batch at epoch 1 : 2.5122151374816895
Loss of batch at epoch 1 : 2.5057196617126465
Loss of batch at epoch 1 : 2.5383429527282715
Loss of batch at epoch 1 : 2.387468099594116
Loss of batch at epoch 1 : 2.400738000869751
Loss of batch at epoch 1 : 2.3373847007751465
Loss of batch at epoch 1 : 2.368393898010254
Loss of batch at epoch 1 : 2.2030701637268066
Loss of batch at epoch 1 : 2.2310032844543457
Loss of batch at epoch 1 : 2.2626900672912598
Loss of batch at epoch 1 : 2.211087703704834
Loss of batch at epoch 1 : 2.188023328781128
Loss of batch at epoch 1 : 2.1553843021392822
Loss of batch at epoch 1 : 2.168257713317871
Loss of batch at epoch 1 : 2.1148056983947754
Loss of batch at epoch 1 : 2.0405642986297607
Loss of batch at epoch 1 : 2.04382586479187
Loss of batch at epoch 1 : 2.084885358810425
Loss of batch at epoch 1 : 2.035876989364624
Loss of batch at epoch 1 : 1.9962064027786255
Loss of batch at epoch 1 : 1.9616570472717285
Loss of batch at epoch 1 : 1.9160150289535522
Loss of batch at epoch 1 : 1.9864258766174316
Loss of batch at epoch 1 : 1.911524772644043
Loss of batch at epoch 1 : 1.8809458017349243
Loss of batch at epoch 1 : 1.8642926216125488
Loss of batch at epoch 1 : 1.857591986656189
Loss of batch at epoch 1 : 1.804038405418396
Loss of batch at epoch 1 : 1.866289496421814
Loss of batch at epoch 1 : 1.8190422058105469
Loss of batch at epoch 1 : 1.8472858667373657
Loss of batch at epoch 1 : 1.9280178546905518
Loss of batch at epoch 1 : 1.7153271436691284
Loss of batch at epoch 1 : 1.925431728363037
Average train loss of epoch 1: 0.049698354594292615
Average validation loss of epoch 1: 0.03673798789078941
Loss of batch at epoch 2 : 1.817721962928772
Loss of batch at epoch 2 : 1.8579316139221191
Loss of batch at epoch 2 : 1.831156611442566
Loss of batch at epoch 2 : 1.860537052154541
Loss of batch at epoch 2 : 1.7684239149093628
Loss of batch at epoch 2 : 1.795999526977539
Loss of batch at epoch 2 : 1.7618460655212402
Loss of batch at epoch 2 : 1.7816669940948486
Loss of batch at epoch 2 : 1.7838820219039917
Loss of batch at epoch 2 : 1.8173636198043823
Loss of batch at epoch 2 : 1.7043343782424927
Loss of batch at epoch 2 : 1.7333810329437256
Loss of batch at epoch 2 : 1.7408697605133057
Loss of batch at epoch 2 : 1.6726504564285278
Loss of batch at epoch 2 : 1.6731940507888794
Loss of batch at epoch 2 : 1.682836890220642
Loss of batch at epoch 2 : 1.5784976482391357
Loss of batch at epoch 2 : 1.6284854412078857
Loss of batch at epoch 2 : 1.6989802122116089
Loss of batch at epoch 2 : 1.7262403964996338
Loss of batch at epoch 2 : 1.7047088146209717
Loss of batch at epoch 2 : 1.665190577507019
Loss of batch at epoch 2 : 1.60579514503479
Loss of batch at epoch 2 : 1.6749061346054077
Loss of batch at epoch 2 : 1.6639608144760132
Loss of batch at epoch 2 : 1.6565865278244019
Loss of batch at epoch 2 : 1.5976512432098389
Loss of batch at epoch 2 : 1.6429989337921143
Loss of batch at epoch 2 : 1.5821629762649536
Loss of batch at epoch 2 : 1.5373852252960205
Loss of batch at epoch 2 : 1.7011888027191162
Loss of batch at epoch 2 : 1.5398865938186646
Loss of batch at epoch 2 : 1.5686733722686768
Loss of batch at epoch 2 : 1.588253140449524
Loss of batch at epoch 2 : 1.577255368232727
Loss of batch at epoch 2 : 1.6942776441574097
Loss of batch at epoch 2 : 1.462484359741211
Loss of batch at epoch 2 : 1.5311113595962524
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 5613946.0 ON gcn17 CANCELLED AT 2024-03-20T10:57:57 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 5613946 ON gcn17 CANCELLED AT 2024-03-20T10:57:57 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: setns failed for /slurm/5613946/.ns: Invalid argument
slurmstepd: error: container_g_join(5613946): Invalid argument

JOB STATISTICS
==============
Job ID: 5613946
Cluster: snellius
User/Group: scur0756/scur0756
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:10
CPU Efficiency: 0.03% of 09:02:06 core-walltime
Job Wall-clock time: 00:30:07
Memory Utilized: 61.47 GB
Memory Efficiency: 17.07% of 360.00 GB
