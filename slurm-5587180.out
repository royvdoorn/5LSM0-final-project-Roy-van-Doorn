wandb: Currently logged in as: r-v-doorn1 (royvdoorn). Use `wandb login --relogin` to force relogin
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.
  warnings.warn(
Loss of batch at epoch 1 : 3.6010067462921143
Loss of batch at epoch 1 : 3.491253614425659
Loss of batch at epoch 1 : 3.420398235321045
Loss of batch at epoch 1 : 3.3388750553131104
Loss of batch at epoch 1 : 3.3146495819091797
Loss of batch at epoch 1 : 3.255345344543457
Loss of batch at epoch 1 : 3.181104898452759
Loss of batch at epoch 1 : 3.1028974056243896
Loss of batch at epoch 1 : 3.085071325302124
Loss of batch at epoch 1 : 3.0050716400146484
Loss of batch at epoch 1 : 2.9221084117889404
Loss of batch at epoch 1 : 2.9507858753204346
Loss of batch at epoch 1 : 2.867121696472168
Loss of batch at epoch 1 : 2.7969729900360107
Loss of batch at epoch 1 : 2.813575029373169
Loss of batch at epoch 1 : 2.739788770675659
Loss of batch at epoch 1 : 2.7271127700805664
Loss of batch at epoch 1 : 2.6553847789764404
Loss of batch at epoch 1 : 2.5883214473724365
Loss of batch at epoch 1 : 2.6278579235076904
Loss of batch at epoch 1 : 2.6053857803344727
Loss of batch at epoch 1 : 2.556448221206665
Loss of batch at epoch 1 : 2.4373457431793213
Loss of batch at epoch 1 : 2.4704480171203613
Loss of batch at epoch 1 : 2.4554738998413086
Loss of batch at epoch 1 : 2.4944069385528564
Loss of batch at epoch 1 : 2.3675835132598877
Loss of batch at epoch 1 : 2.4338812828063965
Loss of batch at epoch 1 : 2.315767765045166
Loss of batch at epoch 1 : 2.3061492443084717
Loss of batch at epoch 1 : 2.2594199180603027
Loss of batch at epoch 1 : 2.2036664485931396
Loss of batch at epoch 1 : 2.314314603805542
Loss of batch at epoch 1 : 2.285057544708252
Loss of batch at epoch 1 : 2.254389762878418
Loss of batch at epoch 1 : 2.2600393295288086
Loss of batch at epoch 1 : 2.2294178009033203
Loss of batch at epoch 1 : 2.203494071960449
Loss of batch at epoch 1 : 2.239184856414795
Loss of batch at epoch 1 : 2.1173617839813232
Loss of batch at epoch 1 : 2.1837079524993896
Loss of batch at epoch 1 : 2.060943841934204
Loss of batch at epoch 1 : 2.0952155590057373
Loss of batch at epoch 1 : 2.0983214378356934
Loss of batch at epoch 1 : 2.1844468116760254
Loss of batch at epoch 1 : 2.1289148330688477
Loss of batch at epoch 1 : 2.097201347351074
Loss of batch at epoch 1 : 2.1383583545684814
Loss of batch at epoch 1 : 2.0410852432250977
Loss of batch at epoch 1 : 2.062915563583374
Loss of batch at epoch 1 : 2.1073338985443115
Loss of batch at epoch 1 : 2.006100654602051
Loss of batch at epoch 1 : 2.017333507537842
Loss of batch at epoch 1 : 2.060633420944214
Loss of batch at epoch 1 : 2.040719509124756
Loss of batch at epoch 1 : 1.9796898365020752
Loss of batch at epoch 1 : 2.1397716999053955
Loss of batch at epoch 1 : 2.0343692302703857
Loss of batch at epoch 1 : 1.9838405847549438
Loss of batch at epoch 1 : 2.0818634033203125
Loss of batch at epoch 1 : 1.987328290939331
Loss of batch at epoch 1 : 2.008208751678467
Loss of batch at epoch 1 : 1.9861193895339966
Loss of batch at epoch 1 : 2.040062665939331
Loss of batch at epoch 1 : 1.9619024991989136
Loss of batch at epoch 1 : 2.019460678100586
Loss of batch at epoch 1 : 1.9593030214309692
Loss of batch at epoch 1 : 1.9559813737869263
Loss of batch at epoch 1 : 1.8741960525512695
Loss of batch at epoch 1 : 1.9259406328201294
Loss of batch at epoch 1 : 1.9975117444992065
Loss of batch at epoch 1 : 1.9149281978607178
Loss of batch at epoch 1 : 1.9395099878311157
Loss of batch at epoch 1 : 1.8670480251312256
Loss of batch at epoch 1 : 1.950608730316162
Loss of batch at epoch 1 : 1.9520994424819946
Loss of batch at epoch 1 : 1.8818659782409668
Loss of batch at epoch 1 : 1.8748034238815308
Loss of batch at epoch 1 : 1.9111063480377197
Loss of batch at epoch 1 : 1.8620312213897705
Loss of batch at epoch 1 : 1.905320167541504
Loss of batch at epoch 1 : 1.884070634841919
Loss of batch at epoch 1 : 1.857852816581726
Loss of batch at epoch 1 : 1.9065544605255127
Loss of batch at epoch 1 : 1.856644868850708
Loss of batch at epoch 1 : 1.8577817678451538
Loss of batch at epoch 1 : 1.798939824104309
Loss of batch at epoch 1 : 1.798183798789978
Loss of batch at epoch 1 : 1.784835696220398
Loss of batch at epoch 1 : 1.8095529079437256
Loss of batch at epoch 1 : 1.8095557689666748
Loss of batch at epoch 1 : 1.817677617073059
Loss of batch at epoch 1 : 1.8041789531707764
Loss of batch at epoch 1 : 1.73747718334198
Loss of batch at epoch 1 : 1.7293566465377808
Loss of batch at epoch 1 : 1.7605628967285156
Loss of batch at epoch 1 : 1.7908321619033813
Loss of batch at epoch 1 : 1.8261051177978516
Loss of batch at epoch 1 : 1.6962988376617432
Loss of batch at epoch 1 : 1.709033489227295
Loss of batch at epoch 1 : 1.6844899654388428
Loss of batch at epoch 1 : 1.7915256023406982
Loss of batch at epoch 1 : 1.7669320106506348
Loss of batch at epoch 1 : 1.7221417427062988
Loss of batch at epoch 1 : 1.8307422399520874
Loss of batch at epoch 1 : 1.7661751508712769
Loss of batch at epoch 1 : 1.805450439453125
Loss of batch at epoch 1 : 1.772255778312683
Average train loss of epoch 1: 0.08886753506763735
Average validation loss of epoch 1: 0.06999763334640349
Loss of batch at epoch 2 : 1.7836133241653442
Loss of batch at epoch 2 : 1.6995567083358765
Loss of batch at epoch 2 : 1.6611294746398926
Loss of batch at epoch 2 : 1.7048051357269287
Loss of batch at epoch 2 : 1.6877444982528687
Loss of batch at epoch 2 : 1.7588149309158325
Loss of batch at epoch 2 : 1.7705646753311157
Loss of batch at epoch 2 : 1.6979795694351196
Loss of batch at epoch 2 : 1.7587742805480957
Loss of batch at epoch 2 : 1.6230961084365845
Loss of batch at epoch 2 : 1.7888562679290771
Loss of batch at epoch 2 : 1.746393084526062
Loss of batch at epoch 2 : 1.7165143489837646
Loss of batch at epoch 2 : 1.70956552028656
Loss of batch at epoch 2 : 1.6592674255371094
Loss of batch at epoch 2 : 1.6570488214492798
Loss of batch at epoch 2 : 1.687729835510254
Loss of batch at epoch 2 : 1.6761237382888794
Loss of batch at epoch 2 : 1.6468167304992676
Loss of batch at epoch 2 : 1.6574695110321045
Loss of batch at epoch 2 : 1.7153468132019043
Loss of batch at epoch 2 : 1.601935625076294
Loss of batch at epoch 2 : 1.750085473060608
Loss of batch at epoch 2 : 1.675263524055481
Loss of batch at epoch 2 : 1.6382075548171997
Loss of batch at epoch 2 : 1.6316826343536377
Loss of batch at epoch 2 : 1.5844368934631348
Loss of batch at epoch 2 : 1.6180928945541382
Loss of batch at epoch 2 : 1.5945731401443481
Loss of batch at epoch 2 : 1.6384658813476562
Loss of batch at epoch 2 : 1.5916293859481812
Loss of batch at epoch 2 : 1.516113519668579
Loss of batch at epoch 2 : 1.557883620262146
Loss of batch at epoch 2 : 1.5977932214736938
Loss of batch at epoch 2 : 1.617653727531433
Loss of batch at epoch 2 : 1.614638328552246
Loss of batch at epoch 2 : 1.6210544109344482
slurmstepd: error: *** JOB 5587180 ON gcn66 CANCELLED AT 2024-03-18T15:55:45 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 5587180.0 ON gcn66 CANCELLED AT 2024-03-18T15:55:45 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: open failed for /slurm/5587180/.ns: No such file or directory
slurmstepd: error: container_g_join(5587180): No such file or directory

JOB STATISTICS
==============
Job ID: 5587180
Cluster: snellius
User/Group: scur0756/scur0756
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 09:03:54 core-walltime
Job Wall-clock time: 00:30:13
Memory Utilized: 7.39 MB
Memory Efficiency: 0.00% of 360.00 GB
